<!--
    Oozie workflow to execute the following actions:

    1) Run Hive action "copy_to_entries", which will:
       a) create an external table pointing to a directory where Ballista dumps data
       b) read from the external table and write to a Hive partitioned table
       c) drops the external table
    2) Rename the directory containing files that were successfully processed
    3) Email notification to emailToAddress for any errors only

    The workflow takes the following parameters:
    
    - region         - the region to process data from. The region name is used
                       as part of a directory path where Ballista is expected
                       to write files in. 
    - emailToAddress - the email address to send error notifications. 
    - dateToProcess  - this is the date (in the format of yyyy-MM-dd) for which
                       this workflow needs to grab exported data from Ballista
                       and writes to Hive. The dateToProcess is also being used
                       as a directory name.

    The directory from which this workflow expects files from Ballista is:
        /cloudfeeds/feeds_dump/${region}/${YEAR}-${MONTH}-${DAY} 
-->
<workflow-app name="usmu" 
	      xmlns="uri:oozie:workflow:0.4">

  <parameters>
    <property>
       <name>region</name>
    </property>
    <property>
       <name>emailToAddress</name>
    </property>
    <property>
       <name>dateToProcess</name>
    </property>
  </parameters>

  <start to="copy_to_entries"/>

  <action name="copy_to_entries">
    <hive xmlns="uri:oozie:hive-action:0.2">
      <job-tracker>${jobTracker}</job-tracker>
      <name-node>${nameNode}</name-node>
      <!--
         Currently /etc/hive/conf/hive-site.xml needs to be copied into workflow's directory
         and exist on HDFS.
      -->
      <job-xml>hive-site.xml</job-xml>

      <configuration>
	<property>
	  <name>mapred.job.queue.name</name>
	  <value>${queueName}</value>
	</property>
      </configuration>

      <!--
           Hive script which must exist in workflow directory on HDFS.
      -->
      <script>copy_to_entries.q</script>

      <!--
           Parameters referenced within Hive script.
      -->
      <param>INPUT_TABLE=dailyfeedsdump_${region}</param>
      <param>INPUT_LOCATION=/cloudfeeds/feeds_dump/${region}/${dateToProcess}</param>
    </hive>
    
    <ok to="cleanup"/>
    <error to="sendEmailKill"/>
  </action>

  <action name="cleanup">
     <fs>
         <!--
         Someone or some process needs to purge these
         -->
         <move source="${nameNode}/cloudfeeds/feeds_dump/${region}/${dateToProcess}" target="${nameNode}/cloudfeeds/feeds_dump/${region}/${dateToProcess}-${wf:id()}"/>
     </fs>
     <ok to="end"/>
     <error to="sendEmailKill"/>
  </action>

  <action name="sendEmailKill">
    <email xmlns="uri:oozie:email-action:0.1">
      <to>${emailToAddress}</to>
      <subject>Status of workflow ${wf:id()}</subject>
      <body>The workflow ${wf:id()} had issues and was killed. The error message is: ${wf:errorMessage(wf:lastErrorNode())}</body>
    </email>
    <ok to="fail"/>
    <error to="fail"/> 
  </action>

  <kill name="fail">
    <message>error message[${wf:errorMessage(wf:lastErrorNode())}]</message> 
  </kill>

  <end name="end"/>

</workflow-app>
